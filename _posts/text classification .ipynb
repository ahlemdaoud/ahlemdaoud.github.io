{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pickle\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Activation, Dense, Dropout\n",
    "import keras\n",
    "from sklearn.preprocessing import LabelBinarizer\n",
    "import sklearn.datasets as skds\n",
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.19.1\n"
     ]
    }
   ],
   "source": [
    "import sklearn\n",
    "print (sklearn.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For reproducibility\n",
    "np.random.seed(1237)\n",
    " \n",
    "# Source file directory\n",
    "path_train = \"C:\\\\DL\\\\20news-bydate-train\"\n",
    " \n",
    "files_train = skds.load_files(path_train,load_content=False)\n",
    " \n",
    "label_index = files_train.target\n",
    "label_names = files_train.target_names\n",
    "labelled_files = files_train.filenames\n",
    " \n",
    "data_tags = [\"filename\",\"category\",\"news\"]\n",
    "data_list = []\n",
    " \n",
    "# Read and add data from file to a list\n",
    "i=0\n",
    "for f in labelled_files:\n",
    "    data_list.append((f,label_names[label_index[i]],Path(f).read_text()))\n",
    "    i += 1\n",
    " \n",
    "# We have training data available as dictionary filename, category, data\n",
    "data = pd.DataFrame.from_records(data_list, columns=data_tags)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lets take 80% data as training and remaining 20% for test.\n",
    "train_size = int(len(data) * .8)\n",
    " \n",
    "train_posts = data['news'][:train_size]\n",
    "train_tags = data['category'][:train_size]\n",
    "train_files_names = data['filename'][:train_size]\n",
    " \n",
    "test_posts = data['news'][train_size:]\n",
    "test_tags = data['category'][train_size:]\n",
    "test_files_names = data['filename'][train_size:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 20 news groups\n",
    "num_labels = 20\n",
    "vocab_size = 15000\n",
    "batch_size = 100\n",
    " \n",
    "# define Tokenizer with Vocab Size\n",
    "tokenizer = Tokenizer(num_words=vocab_size)\n",
    "tokenizer.fit_on_texts(train_posts)\n",
    " \n",
    "x_train = tokenizer.texts_to_matrix(train_posts, mode='tfidf')\n",
    "x_test = tokenizer.texts_to_matrix(test_posts, mode='tfidf')\n",
    " \n",
    "encoder = LabelBinarizer()\n",
    "encoder.fit(train_tags)\n",
    "y_train = encoder.transform(train_tags)\n",
    "y_test = encoder.transform(test_tags)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder = LabelBinarizer()\n",
    "encoder.fit(train_tags)\n",
    "y_train = encoder.transform(train_tags)\n",
    "y_test = encoder.transform(test_tags)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_6 (Dense)              (None, 512)               7680512   \n",
      "_________________________________________________________________\n",
      "activation_1 (Activation)    (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dense_7 (Dense)              (None, 512)               262656    \n",
      "_________________________________________________________________\n",
      "activation_2 (Activation)    (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dense_8 (Dense)              (None, 20)                10260     \n",
      "_________________________________________________________________\n",
      "activation_3 (Activation)    (None, 20)                0         \n",
      "=================================================================\n",
      "Total params: 7,953,428\n",
      "Trainable params: 7,953,428\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "WARNING:tensorflow:From C:\\ProgramData\\Miniconda3\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:2755: calling reduce_sum (from tensorflow.python.ops.math_ops) with keep_dims is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "keep_dims is deprecated, use keepdims instead\n",
      "WARNING:tensorflow:From C:\\ProgramData\\Miniconda3\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:1290: calling reduce_mean (from tensorflow.python.ops.math_ops) with keep_dims is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "keep_dims is deprecated, use keepdims instead\n",
      "Train on 8145 samples, validate on 906 samples\n",
      "Epoch 1/30\n",
      "8145/8145 [==============================] - 26s - loss: 1.1722 - acc: 0.6961 - val_loss: 0.4389 - val_acc: 0.8753\n",
      "Epoch 2/30\n",
      "8145/8145 [==============================] - 24s - loss: 0.1309 - acc: 0.9713 - val_loss: 0.3876 - val_acc: 0.9040\n",
      "Epoch 3/30\n",
      "8145/8145 [==============================] - 29s - loss: 0.0655 - acc: 0.9899 - val_loss: 0.4257 - val_acc: 0.8996\n",
      "Epoch 4/30\n",
      "8145/8145 [==============================] - 26s - loss: 0.0417 - acc: 0.9941 - val_loss: 0.3923 - val_acc: 0.9029\n",
      "Epoch 5/30\n",
      "8145/8145 [==============================] - 26s - loss: 0.0343 - acc: 0.9966 - val_loss: 0.4350 - val_acc: 0.9040\n",
      "Epoch 6/30\n",
      "8145/8145 [==============================] - 24s - loss: 0.0344 - acc: 0.9961 - val_loss: 0.4054 - val_acc: 0.9018\n",
      "Epoch 7/30\n",
      "8145/8145 [==============================] - 24s - loss: 0.0391 - acc: 0.9964 - val_loss: 0.4063 - val_acc: 0.9095\n",
      "Epoch 8/30\n",
      "8145/8145 [==============================] - 24s - loss: 0.0370 - acc: 0.9961 - val_loss: 0.5284 - val_acc: 0.8896\n",
      "Epoch 9/30\n",
      "8145/8145 [==============================] - 27s - loss: 0.0320 - acc: 0.9964 - val_loss: 0.5047 - val_acc: 0.9007\n",
      "Epoch 10/30\n",
      "8145/8145 [==============================] - 27s - loss: 0.0467 - acc: 0.9958 - val_loss: 0.5221 - val_acc: 0.8940\n",
      "Epoch 11/30\n",
      "8145/8145 [==============================] - 24s - loss: 0.0647 - acc: 0.9928 - val_loss: 0.5836 - val_acc: 0.8929\n",
      "Epoch 12/30\n",
      "8145/8145 [==============================] - 22s - loss: 0.0650 - acc: 0.9932 - val_loss: 0.5858 - val_acc: 0.8797\n",
      "Epoch 13/30\n",
      "8145/8145 [==============================] - 22s - loss: 0.0534 - acc: 0.9950 - val_loss: 0.5821 - val_acc: 0.8852\n",
      "Epoch 14/30\n",
      "8145/8145 [==============================] - 23s - loss: 0.0764 - acc: 0.9915 - val_loss: 0.6383 - val_acc: 0.8720\n",
      "Epoch 15/30\n",
      "8145/8145 [==============================] - 21s - loss: 0.0537 - acc: 0.9934 - val_loss: 0.6456 - val_acc: 0.8885\n",
      "Epoch 16/30\n",
      "8145/8145 [==============================] - 27s - loss: 0.0480 - acc: 0.9950 - val_loss: 0.6784 - val_acc: 0.8786\n",
      "Epoch 17/30\n",
      "8145/8145 [==============================] - 28s - loss: 0.0547 - acc: 0.9945 - val_loss: 0.7535 - val_acc: 0.8852\n",
      "Epoch 18/30\n",
      "8145/8145 [==============================] - 29s - loss: 0.0456 - acc: 0.9948 - val_loss: 0.7704 - val_acc: 0.8687\n",
      "Epoch 19/30\n",
      "8145/8145 [==============================] - 28s - loss: 0.0585 - acc: 0.9928 - val_loss: 0.8894 - val_acc: 0.8675\n",
      "Epoch 20/30\n",
      "8145/8145 [==============================] - 25s - loss: 0.0618 - acc: 0.9919 - val_loss: 1.0075 - val_acc: 0.8598\n",
      "Epoch 21/30\n",
      "8145/8145 [==============================] - 24s - loss: 0.1027 - acc: 0.9864 - val_loss: 1.1404 - val_acc: 0.8477\n",
      "Epoch 22/30\n",
      "8145/8145 [==============================] - 22s - loss: 0.0956 - acc: 0.9864 - val_loss: 1.0709 - val_acc: 0.8488\n",
      "Epoch 23/30\n",
      "8145/8145 [==============================] - 23s - loss: 0.0728 - acc: 0.9907 - val_loss: 0.9600 - val_acc: 0.8687\n",
      "Epoch 24/30\n",
      "8145/8145 [==============================] - 27s - loss: 0.0621 - acc: 0.9923 - val_loss: 0.9050 - val_acc: 0.8753\n",
      "Epoch 25/30\n",
      "8145/8145 [==============================] - 27s - loss: 0.0458 - acc: 0.9950 - val_loss: 0.8734 - val_acc: 0.8720\n",
      "Epoch 26/30\n",
      "8145/8145 [==============================] - 25s - loss: 0.0450 - acc: 0.9953 - val_loss: 0.9388 - val_acc: 0.8742\n",
      "Epoch 27/30\n",
      "8145/8145 [==============================] - 26s - loss: 0.0448 - acc: 0.9953 - val_loss: 0.9418 - val_acc: 0.8753\n",
      "Epoch 28/30\n",
      "8145/8145 [==============================] - 26s - loss: 0.0370 - acc: 0.9963 - val_loss: 0.8573 - val_acc: 0.8808\n",
      "Epoch 29/30\n",
      "8145/8145 [==============================] - 25s - loss: 0.0531 - acc: 0.9944 - val_loss: 0.9806 - val_acc: 0.8742\n",
      "Epoch 30/30\n",
      "8145/8145 [==============================] - 25s - loss: 0.0860 - acc: 0.9901 - val_loss: 0.9687 - val_acc: 0.8753\n"
     ]
    }
   ],
   "source": [
    "model = Sequential()\n",
    "model.add(Dense(512, input_shape=(vocab_size,)))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dropout(0.3))\n",
    "model.add(Dense(512))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dropout(0.3))\n",
    "model.add(Dense(num_labels))\n",
    "model.add(Activation('softmax'))\n",
    "model.summary()\n",
    " \n",
    "model.compile(loss='categorical_crossentropy',\n",
    "              optimizer='adam',\n",
    "              metrics=['accuracy'])\n",
    " \n",
    "history = model.fit(x_train, y_train,\n",
    "                    batch_size=batch_size,\n",
    "                    epochs=30,\n",
    "                    verbose=1,\n",
    "                    validation_split=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2200/2263 [============================>.] - ETA: 0sTest accuracy: 0.8718515261052933\n",
      "C:\\DL\\20news-bydate-train\\alt.atheism\\53114\n",
      "Actual label:alt.atheism\n",
      "Predicted label: alt.atheism\n",
      "C:\\DL\\20news-bydate-train\\comp.graphics\\38666\n",
      "Actual label:comp.graphics\n",
      "Predicted label: comp.graphics\n",
      "C:\\DL\\20news-bydate-train\\sci.med\\58932\n",
      "Actual label:sci.med\n",
      "Predicted label: sci.med\n",
      "C:\\DL\\20news-bydate-train\\sci.crypt\\15212\n",
      "Actual label:sci.crypt\n",
      "Predicted label: sci.crypt\n",
      "C:\\DL\\20news-bydate-train\\comp.os.ms-windows.misc\\9695\n",
      "Actual label:comp.os.ms-windows.misc\n",
      "Predicted label: comp.os.ms-windows.misc\n",
      "C:\\DL\\20news-bydate-train\\rec.sport.baseball\\104482\n",
      "Actual label:rec.sport.baseball\n",
      "Predicted label: rec.sport.baseball\n",
      "C:\\DL\\20news-bydate-train\\soc.religion.christian\\20731\n",
      "Actual label:soc.religion.christian\n",
      "Predicted label: sci.med\n",
      "C:\\DL\\20news-bydate-train\\comp.graphics\\38583\n",
      "Actual label:comp.graphics\n",
      "Predicted label: comp.graphics\n",
      "C:\\DL\\20news-bydate-train\\rec.sport.hockey\\52638\n",
      "Actual label:rec.sport.hockey\n",
      "Predicted label: rec.sport.hockey\n",
      "C:\\DL\\20news-bydate-train\\rec.sport.hockey\\52636\n",
      "Actual label:rec.sport.hockey\n",
      "Predicted label: rec.sport.hockey\n"
     ]
    }
   ],
   "source": [
    "score = model.evaluate(x_test, y_test,\n",
    "                       batch_size=batch_size, verbose=1)\n",
    " \n",
    "print('Test accuracy:', score[1])\n",
    " \n",
    "text_labels = encoder.classes_\n",
    " \n",
    "for i in range(10):\n",
    "    prediction = model.predict(np.array([x_test[i]]))\n",
    "    predicted_label = text_labels[np.argmax(prediction[0])]\n",
    "    print(test_files_names.iloc[i])\n",
    "    print('Actual label:' + test_tags.iloc[i])\n",
    "    print(\"Predicted label: \" + predicted_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.model.save('my_model.h5')\n",
    " \n",
    "# Save Tokenizer i.e. Vocabulary\n",
    "with open('tokenizer.pickle', 'wb') as handle:\n",
    "    pickle.dump(tokenizer, handle, protocol=pickle.HIGHEST_PROTOCOL)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = keras.models.load_model('my_model.h5')\n",
    "# load tokenizer\n",
    "tokenizer = Tokenizer()\n",
    "with open('tokenizer.pickle', 'rb') as handle:\n",
    "    tokenizer = pickle.load(handle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['alt.atheism', 'comp.graphics', 'comp.os.ms-windows.misc',\n",
       "       'comp.sys.ibm.pc.hardware', 'comp.sys.mac.hardware',\n",
       "       'comp.windows.x', 'misc.forsale', 'rec.autos', 'rec.motorcycles',\n",
       "       'rec.sport.baseball', 'rec.sport.hockey', 'sci.crypt',\n",
       "       'sci.electronics', 'sci.med', 'sci.space',\n",
       "       'soc.religion.christian', 'talk.politics.guns',\n",
       "       'talk.politics.mideast', 'talk.politics.misc',\n",
       "       'talk.religion.misc'], dtype='<U24')"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoder.classes_ #LabelBinarizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File -> C:\\DL\\20news-bydate-test\\comp.graphics\\38758 Predicted label: comp.graphics\n",
      "File -> C:\\DL\\20news-bydate-test\\misc.forsale\\76115 Predicted label: comp.sys.ibm.pc.hardware\n",
      "File -> C:\\DL\\20news-bydate-test\\soc.religion.christian\\21329 Predicted label: soc.religion.christian\n"
     ]
    }
   ],
   "source": [
    "# These are the labels we stored from our training\n",
    "# The order is very important here.\n",
    " \n",
    "labels = np.array(['alt.atheism', 'comp.graphics', 'comp.os.ms-windows.misc',\n",
    " 'comp.sys.ibm.pc.hardware', 'comp.sys.mac.hardware', 'comp.windows.x',\n",
    " 'misc.forsale', 'rec.autos', 'rec.motorcycles', 'rec.sport.baseball',\n",
    " 'rec.sport.hockey', 'sci.crypt', 'sci.electronics', 'sci.med', 'sci.space',\n",
    " 'soc.religion.christian', 'talk.politics.guns', 'talk.politics.mideast',\n",
    " 'talk.politics.misc', 'talk.religion.misc'])\n",
    " \n",
    "test_files = [\"C:\\\\DL\\\\20news-bydate-test\\\\comp.graphics\\\\38758\",\n",
    "              \"C:\\\\DL\\\\20news-bydate-test\\\\misc.forsale\\\\76115\",\n",
    "              \"C:\\\\DL\\\\20news-bydate-test\\\\soc.religion.christian\\\\21329\"\n",
    "              ]\n",
    "x_data = []\n",
    "for t_f in test_files:\n",
    "    t_f_data = Path(t_f).read_text()\n",
    "    x_data.append(t_f_data)\n",
    " \n",
    "x_data_series = pd.Series(x_data)\n",
    "x_tokenized = tokenizer.texts_to_matrix(x_data_series, mode='tfidf')\n",
    " \n",
    "i=0\n",
    "for x_t in x_tokenized:\n",
    "    prediction = model.predict(np.array([x_t]))\n",
    "    predicted_label = labels[np.argmax(prediction[0])]\n",
    "    print(\"File ->\", test_files[i], \"Predicted label: \" + predicted_label)\n",
    "    i += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
