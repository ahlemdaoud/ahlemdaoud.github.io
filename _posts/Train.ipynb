{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "import numpy\n",
    "from time import time \n",
    "from keras import callbacks\n",
    "from keras.callbacks import TensorBoard\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Conv2D, MaxPooling2D\n",
    "from keras.layers import Activation, Dropout, Flatten, Dense\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "datagen = ImageDataGenerator(\n",
    "        rotation_range=40,\n",
    "        width_shift_range=0.2,\n",
    "        height_shift_range=0.2,\n",
    "        rescale=1./255,\n",
    "        shear_range=0.2,\n",
    "        zoom_range=0.2,\n",
    "        horizontal_flip=True,\n",
    "        fill_mode='nearest')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_1 (Conv2D)            (None, 148, 148, 32)      896       \n",
      "_________________________________________________________________\n",
      "activation_1 (Activation)    (None, 148, 148, 32)      0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_1 (MaxPooling2 (None, 74, 74, 32)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_2 (Conv2D)            (None, 72, 72, 32)        9248      \n",
      "_________________________________________________________________\n",
      "activation_2 (Activation)    (None, 72, 72, 32)        0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_2 (MaxPooling2 (None, 36, 36, 32)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_3 (Conv2D)            (None, 34, 34, 64)        18496     \n",
      "_________________________________________________________________\n",
      "activation_3 (Activation)    (None, 34, 34, 64)        0         \n",
      "=================================================================\n",
      "Total params: 28,640\n",
      "Trainable params: 28,640\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "\n",
    "model = Sequential()\n",
    "model.add(Conv2D(32, (3, 3), input_shape=(150,150,3))), \n",
    "model.add(Activation('relu'))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "\n",
    "model.add(Conv2D(32, (3, 3)))\n",
    "model.add(Activation('relu'))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "\n",
    "model.add(Conv2D(64, (3, 3)))\n",
    "model.add(Activation('relu'))\n",
    "model.summary()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\ProgramData\\Miniconda3\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:1205: calling reduce_prod (from tensorflow.python.ops.math_ops) with keep_dims is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "keep_dims is deprecated, use keepdims instead\n"
     ]
    }
   ],
   "source": [
    "model.add(Flatten())  # this converts our 3D feature maps to 1D feature vectors\n",
    "model.add(Dense(64))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(1))\n",
    "model.add(Activation('sigmoid'))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_json = model.to_json()\n",
    "with open(\"tt.json\", \"w\") as json_file:\n",
    "    json_file.write(model_json)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_json = model.to_json()\n",
    "with open(\"tt.json\", \"w\") as json_file:\n",
    "    json_file.write(model_json)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_datagen = ImageDataGenerator(\n",
    "        rescale=1./255,\n",
    "        shear_range=0.2,\n",
    "        zoom_range=0.2,\n",
    "        horizontal_flip=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 20000 images belonging to 2 classes.\n"
     ]
    }
   ],
   "source": [
    "train_generator = train_datagen.flow_from_directory(\n",
    "        'data/train',  # this is the target directory\n",
    "        target_size=(150, 150),  # all images will be resized to 150x150\n",
    "        batch_size=batch_size,\n",
    "        class_mode='binary') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_datagen = ImageDataGenerator(rescale=1./255)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 5000 images belonging to 2 classes.\n"
     ]
    }
   ],
   "source": [
    "validation_generator = test_datagen.flow_from_directory(\n",
    "        'data/Validation',\n",
    "        target_size=(150, 150),\n",
    "        batch_size=batch_size,\n",
    "        class_mode='binary')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\ProgramData\\Miniconda3\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:1290: calling reduce_mean (from tensorflow.python.ops.math_ops) with keep_dims is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "keep_dims is deprecated, use keepdims instead\n"
     ]
    }
   ],
   "source": [
    "model.compile(loss='binary_crossentropy',\n",
    "              optimizer='rmsprop',\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "125/125 [==============================] - 121s - loss: 0.6736 - acc: 0.5970 - val_loss: 0.6489 - val_acc: 0.6238\n",
      "Epoch 2/10\n",
      "125/125 [==============================] - 129s - loss: 0.6495 - acc: 0.6305 - val_loss: 0.5698 - val_acc: 0.7262\n",
      "Epoch 3/10\n",
      "125/125 [==============================] - 121s - loss: 0.6131 - acc: 0.6955 - val_loss: 0.5403 - val_acc: 0.7312\n",
      "Epoch 4/10\n",
      "125/125 [==============================] - 128s - loss: 0.6250 - acc: 0.6680 - val_loss: 0.5818 - val_acc: 0.6813\n",
      "Epoch 5/10\n",
      "125/125 [==============================] - 120s - loss: 0.6064 - acc: 0.6950 - val_loss: 0.5868 - val_acc: 0.6982\n",
      "Epoch 6/10\n",
      "125/125 [==============================] - 110s - loss: 0.5985 - acc: 0.6825 - val_loss: 0.5809 - val_acc: 0.6837\n",
      "Epoch 7/10\n",
      "125/125 [==============================] - 109s - loss: 0.6169 - acc: 0.6735 - val_loss: 0.5220 - val_acc: 0.7362\n",
      "Epoch 8/10\n",
      "125/125 [==============================] - 109s - loss: 0.5923 - acc: 0.6985 - val_loss: 0.5304 - val_acc: 0.7325\n",
      "Epoch 9/10\n",
      "125/125 [==============================] - 111s - loss: 0.5760 - acc: 0.7115 - val_loss: 0.5214 - val_acc: 0.7450\n",
      "Epoch 10/10\n",
      "125/125 [==============================] - 152s - loss: 0.5710 - acc: 0.7080 - val_loss: 0.5225 - val_acc: 0.7462\n"
     ]
    }
   ],
   "source": [
    "model.fit_generator(\n",
    "        train_generator,\n",
    "        steps_per_epoch=2000 // batch_size,\n",
    "        epochs=10,\n",
    "        validation_data=validation_generator,\n",
    "        validation_steps=800 // batch_size)\n",
    "model.save_weights('first_try.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "125/125 [==============================] - 308s - loss: 0.5933 - acc: 0.7060 - val_loss: 0.5354 - val_acc: 0.7344\n",
      "Epoch 2/10\n",
      "125/125 [==============================] - 295s - loss: 0.5547 - acc: 0.7345 - val_loss: 0.5386 - val_acc: 0.7369\n",
      "Epoch 3/10\n",
      "125/125 [==============================] - 291s - loss: 0.5605 - acc: 0.7130 - val_loss: 0.5818 - val_acc: 0.7017\n",
      "Epoch 4/10\n",
      "125/125 [==============================] - 284s - loss: 0.5929 - acc: 0.7210 - val_loss: 0.5971 - val_acc: 0.6886\n",
      "Epoch 5/10\n",
      "125/125 [==============================] - 286s - loss: 0.5629 - acc: 0.7305 - val_loss: 0.5782 - val_acc: 0.7089\n",
      "Epoch 6/10\n",
      "125/125 [==============================] - 317s - loss: 0.5692 - acc: 0.7240 - val_loss: 0.5154 - val_acc: 0.7409\n",
      "Epoch 7/10\n",
      "125/125 [==============================] - 301s - loss: 0.5596 - acc: 0.7160 - val_loss: 0.5138 - val_acc: 0.7403\n",
      "Epoch 8/10\n",
      "124/125 [============================>.] - ETA: 0s - loss: 0.5628 - acc: 0.7263"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# prepare log path to tensorbord\n",
    "model_tensorboard_log ='training_log/'\n",
    "\n",
    "# assign log path to tensorbord\n",
    "\n",
    "tensorboard = TensorBoard(log_dir=model_tensorboard_log + \"{}\".format(time()))\n",
    "# add tensorbord to the callback list\n",
    "\n",
    "# the fit ot fit_generator function has callbacks as input parameter\n",
    "callbacks_list = [tensorboard]\n",
    "model.fit_generator(train_generator,\n",
    "        steps_per_epoch=2000 // batch_size,\n",
    "        epochs=10,\n",
    "        validation_data=validation_generator,\n",
    "        validation_steps=800,\n",
    "        callbacks=callbacks_list )\n",
    "model.save_weights('first_try.h5')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "################ load model ########\n",
    "def load_model():\n",
    "    print(\"===================== load model =========================\")\n",
    "    json_file = open(\"model.json\", 'r')\n",
    "    loaded_model_json = json_file.read()\n",
    "    json_file.close()\n",
    "    loaded_model = model_from_json(loaded_model_json)\n",
    "    # load weights into new model\n",
    "    loaded_model.load_weights(\"model.h5\")\n",
    "    print(\"Loaded model from disk\")\n",
    "    return loaded_model\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
